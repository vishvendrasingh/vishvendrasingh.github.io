---
layout: post
status: publish
published: true
title: Completed coding of recursive crawler
author:
  display_name: vishvendra
  login: vishvendra
  email: vishvendrasingh1993@gmail.com
  url: ''
author_login: vishvendra
author_email: vishvendrasingh1993@gmail.com
wordpress_id: 165
wordpress_url: http://www.vishvendrasingh.com/?p=165
date: '2016-04-19 16:41:26 +0530'
date_gmt: '2016-04-19 16:41:26 +0530'
categories:
- Projects
tags:
- Common
- Crawler
comments: []
---
<p>Completed coding of recursive crawler, it was fun and a lot of hard work, some meditation, and lots of google. I finally did it. My friend Abhijeet asked to make recursive crawler and I was thinking how can I do that. So came up with this idea wo making two lists</p>
<p>1. processed list (All crawled urls are stored here)</p>
<p>2. unprocessed list (All new url are stored here)</p>
<p>Now if a new url exists in any of these lists then skip it and move furthur. Happy crawling guys.....:)</p>
<p>This program do the following thing</p>
<ol>
<li>store data in mongodb</li>
<li>parse html in page title, meta data, meta keywords</li>
<li>In case if page request fails error handling save it from breaking</li>
<li>it does not follow any other domain except the given one</li>
</ol>
<p>Here is the link <a href="https://github.com/vishvendrasingh/crawler.git" target="_blank">https://github.com/vishvendrasingh/crawler.git</a></p>
